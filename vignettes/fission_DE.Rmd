---
title: "hpgltools Differential Expression Analyses Using the Fission Dataset"
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{fission examples}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Example hpgltool usage with a real data set (fission)

This document aims to provide further examples in how to use the hpgltools.

Note to self, the header has rmarkdown::pdf_document instead of html_document or html_vignette
because it gets some bullcrap error 'margins too large'...

## Setting up

Here are the commands I invoke to get ready to play with new data, including everything
required to install hpgltools, the software it uses, and the fission data.

```{r setup, include=TRUE}

## These first 4 lines are not needed once hpgltools is installed.
## source("http://bioconductor.org/biocLite.R")
## biocLite("devtools")
## library(devtools)
## install_github("elsayed-lab/hpgltools")
library(hpgltools)
require.auto("fission")
library(fission)
data(fission)
knitr::opts_knit$set(progress=TRUE, verbose=TRUE, error=TRUE,  fig.width=7, fig.height=7)

```

## Data import

All the work I do in Dr. El-Sayed's lab makes some pretty hard
assumptions about how data is stored.  As a result, to use the fission
data set I will do a little bit of shenanigans to match it to the
expected format.  Now that I have played a little with fission, I
think its format is quite nice and am likely to have my experiment
class instead be a SummarizedExperiment.

```{r data_import}

## Extract the meta data from the fission dataset
meta <- as.data.frame(fission@colData)
## Make conditions and batches
meta$condition <- paste(meta$strain, meta$minute, sep=".")
meta$batch <- meta$replicate
meta$sample.id <- rownames(meta)
## Grab the count data
fission_data <- fission@assays$data$counts
## This will make an experiment superclass called 'expt' and it contains
## an ExpressionSet along with any arbitrary additional information one might want to include.
## Along the way it writes a Rdata file which is by default called 'expt.Rdata'
fission_expt <- create_expt(meta_dataframe=meta, count_dataframe=fission_data)

```

# Some simple differential expression analyses

Travis wisely imposes a limit on the amount of time for building vignettes.
My tools by default will attempt all possible pairwise comparisons, which takes a long time.
Therefore I am going to take a subset of the data and limit these comparisons to that.

```{r simple_subset}

fun_data <- expt_subset(fission_expt, subset="condition=='wt.120'|condition=='mut.120'")
fun_norm <- normalize_expt(fun_data, batch="limma", norm="quant", transform="log2", convert="cpm")

```

## Try using limma first

```{r simple_limma}

limma_comparison <- limma_pairwise(fun_norm)
names(limma_comparison$all_tables)
summary(limma_comparison$all_tables$wt.120_vs_mut.120)
wt_120 <- limma_comparison$all_tables$wt.120
mut_120 <- limma_comparison$all_tables$mut.120
scatter_wt_mut <- limma_coefficient_scatter(limma_comparison, x="wt.120", y="mut.120", gvis_filename=NULL)
scatter_wt_mut$scatter
scatter_wt_mut$both_histogram

```

## Then DESeq2

```{r simple_deseq2}

deseq_comparison <- deseq2_pairwise(fun_data, model_batch=TRUE)
summary(deseq_comparison$all_tables$wt.120_vs_mut.120)

```

## And EdgeR

```{r simple_edger}

edger_comparison <- edger_pairwise(fun_data, model_batch=TRUE)
summary(edger_comparison$all_tables$wt.120_vs_mut.120)

```

## My stupid basic comparison

```{r simple_basic}

basic_comparison <- basic_pairwise(fun_data)
summary(basic_comparison$all_tables$wt.120_vs_mut.120)

```

## Combine them all

```{r simple_all}

all_comparisons <- all_pairwise(fun_data, model_batch=TRUE)
all_combined <- combine_de_tables(all_comparisons)
sig_genes <- extract_significant_genes(all_combined, excel=NULL)

```

## Ontology searches

The following works, but on travis to causes the vignette build
time to exceed the maximum allowed, which is weird because on my computer
it only takes like 4 minutes.

### Setting up

Since I didn't acquire this data in a 'normal' way, I am going to post-generate a
gff file which may be used by clusterprofiler, topgo, and gostats.

Therefore, I am going to make use of TxDb to make the requisite gff file.

```{r ontology_setup}

limma_results <- limma_comparison$all_tables
## The set of comparisons performed
names(limma_results)
table <- limma_results$wt.120_vs_mut.120
dim(table)
gene_names <- rownames(table)

updown_genes <- get_sig_genes(table)
require.auto("GenomicFeatures")
require.auto("biomaRt")
ensembl_pombe <- biomaRt::useMart("fungal_mart", dataset="spombe_eg_gene", host="fungi.ensembl.org")
pombe_filters <- biomaRt::listFilters(ensembl_pombe)
head(pombe_filters, n=20) ## 11 looks to be my guy

possible_pombe_attributes <- biomaRt::listAttributes(ensembl_pombe)
##pombe_goids <- biomaRt::getBM(attributes=c('pombase_gene_name', 'go_accession'), filters="biotype", values=gene_names, mart=ensembl_pombe)
pombe_goids <- biomaRt::getBM(attributes=c('pombase_transcript', 'go_accession'), values=gene_names, mart=ensembl_pombe)
colnames(pombe_goids) <- c("ID","GO")

pombe <- GenomicFeatures::makeTxDbFromBiomart(biomart ="fungal_mart", dataset = "spombe_eg_gene", host="fungi.ensembl.org")
pombe_transcripts <- as.data.frame(GenomicFeatures::transcriptsBy(pombe))
lengths <- pombe_transcripts[,c("group_name","width")]
colnames(lengths) <- c("ID","width")
## Something useful I didn't notice before:
## makeTranscriptDbFromGFF()  ## From GenomicFeatures, much like my own gff2df()
gff_from_txdb <- GenomicFeatures::asGFF(pombe)
## why is GeneID: getting prefixed to the IDs!?
gff_from_txdb$ID <- gsub(x=gff_from_txdb$ID, pattern="GeneID:", replacement="")
written_gff <- rtracklayer::export.gff3(gff_from_txdb, con="pombe.gff")

```

## Perform a simplified goseq search

Lets try a simplified goseq search using out population of differentially expressed genes, gene lengths, and GOids.

```{r simple_goseq}

goseq_search <- simple_goseq(de_genes=updown_genes$up_genes, lengths=lengths, goids=pombe_goids)
goseq_search$pvalue_histogram + ggplot2::scale_y_discrete(limits=c(0, 10))
goseq_search$bpp_plot

```

## Perform a simplified clusterprofiler search

The first time this is run for a new species, it takes _forever_ because of the
time required to generate the GO<->ID rda files.
That will never float on travis, so I am commenting this out.

```{r simple_clusterprofiler, eval=FALSE}

cluster_search = simple_clusterprofiler(de_genes=updown_genes$up_genes, goids=pombe_goids, gff="pombe.gff")
cluster_search$bp_group_barplot
cluster_search$bp_all_barplot

```

## Simplified topgo

Topgo also takes forever, you will just have to trust me that it works I guess.

```{r simple_topgo, eval=FALSE}

topgo_search = simple_topgo(de_genes=updown_genes$up_genes, goids_df=pombe_goids)
topgo_search$pvalue_plots$BP
head(topgo_search$tables$bp)

```

## Simplified gostats

This too takes quite a long time.  Maybe I should drop it too?

```{r simple_gostats, eval=FALSE}

gostats_search = simple_gostats(updown_genes$up_genes, "pombe.gff", pombe_goids, gff_type="gene")
## Oops I forgot to add my 25 character wrapper for these plots, whatever
## that is weird I thought I did!
gostats_search$pvalue_plots$mfp_plot_over
head(gostats_search$mf_over_all)

```
