---
title: "Hpgltools examples using a small bacterial data set."
author: "atb abelew@gmail.com"
date: "`r Sys.Date()`"
output:
 html_document:
  theme: cosmo
  highlight: tango
  fig_height: 7
  fig_width: 7
  fig_caption: true
  code_folding: show
  self_contained: true
  keep_md: true
  toc: true
  toc_float:
    collapsed: false
    smooth_scroll: false
vignette: >
  %\VignetteIndexEntry{a-01_bacterial_example}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r options, include=FALSE}
## These are the options I tend to favor
library("hpgltools")
knitr::opts_knit$set(
    progress = TRUE,
    verbose = TRUE,
    width = 90,
    echo = TRUE)
knitr::opts_chunk$set(
    error = TRUE,
    fig.width = 8,
    fig.height = 8,
    dpi = 96)
options(
    digits = 4,
    stringsAsFactors = FALSE,
    knitr.duplicate.label = "allow")
ggplot2::theme_set(ggplot2::theme_bw(base_size=10))
set.seed(1)
rmd_file <- "a-01_bacterial_example.Rmd"
```

```{r rendering, include=FALSE, eval=FALSE}
## This block is used to render a document from within it.
rmarkdown::render(rmd_file)

## An extra renderer for pdf output
rmarkdown::render(rmd_file, output_format="pdf_document", output_options=c("skip_html"))

## Or to save/load large Rdata files.
hpgltools:::saveme()
hpgltools:::loadme()
rm(list=ls())
```

# Introduction

hpgltools was written to make working with high-throughput data
analyses easier. These analyses generally fall into a few stages:

1.  Data visualization and outlier/batch evaluation
2.  Differential expression analyses
 a. Visualization and export of these results
3.  Gene ontology/KEGG analyses
 a. Visualization and export of these results
4.  Genome visualizations
 a. With circos
 b. With genoplotR

Before any of these tasks may be performed, the data must be loaded
into memory.  hpgltools attempts to make this easier with
create_expt() and subset_expt().

# Loading (meta)data and annotations

The following examples will use a real data set from a recent
experiment in our lab.  The raw data was processed using a mix of
trimmomatic, biopieces, bowtie, samtools, and htseq.  The final count
tables were deposited into the 'preprocessing/count_tables/' tree.
The resulting data structure was named 'most_v0M1,' named because it
is comprised of count tables with 0 mismatches and 1 randomly-placed
multi-match.

The annotation file was mgas_5005.gff.xz resides in
'reference/gff/'.

The count tables and meta-data were loaded through the create_expt()
function and the genome annotations were loaded with gff2df().

```{r loading_data}
library(hpgltools)
data_file <- system.file("cdm_expt.rda", package="hpgltools")
cdm <- new.env()
load(data_file, envir=cdm)
rm(data_file)

ls()
## 2 variables should exist now: rmd_file in case I want to knitr this file, cdm which is a list including the data required to make an expressionset.

## expt <- create_expt(metadata='filename.xlsx', gene_info=annotation_data)
expt <- create_expt(count_dataframe=cdm$cdm_counts, metadata=cdm$cdm_metadata, gene_info=cdm$gene_info)
## The gff information is in 'annotations'
## The experiment is in most_v0M1
## Here is the meta-data! (well, the first 6 lines anyway).
knitr::kable(head(expt$design))
summary(expt)
```

The data structure generated by create_expt() is a list containing the
following slots:

* initial_metadata:        A backup of the metadata
* original_expressionset:  A backup of the raw counts
* expressionset:           The current count data
* samples:                 A data frame of metadata used for subsets
* design:                  The design of the experiment
* definitions:             Extended design information, these are
  probably redundant and should be pruned.
* stages:                  The experimental stage
* types:                   Cell types
* conditions:              Experimental condition
* batches:                 Experimental batch
* samplenames:             Names of the samples
* colors:                  Colors chosen for graphs and such
* names:                   Bringing together the condition/batch
* filtered:                low-count filtering status of the counts
* transform:               transformation applied to the counts
* norm:                    normalization applied to the counts
* convert:                 cpm/rpkm/etc applied to the data
* original_libsize:        the library sizes before normalization
* columns:                 A backup of the sample names

# Raw metrics

One possibility would be to examine the data in its unmolested state:

```{r graph_original, show.fig="hide"}
raw_metrics <- sm(graph_metrics(expt, qq=TRUE))
```

The function graph_metrics() performs all of the likely plots one might want.  Some of which are not
really appropriate for non-normalized data unless it is incredibly well behaved (after 30 years, I
still want to spell behaved 'behaived', why is that?).

```{r show_original_plots}
## View a raw library size plot
raw_metrics$libsize
## Or boxplot to see the data distribution
raw_metrics$boxplot
## The warning is because it automatically uses a log scale and there are some 0 count genes.
## Perhaps you prefer density plots
raw_metrics$density
## quantile/quantile plots compared to the median of all samples
raw_metrics$qqrat
## Here we can see some samples are differently 'shaped' compared to the median than others
## There are other plots one may view, but this data set is a bit too crowded as is.
## The following summary shows the other available plots:
summary(raw_metrics)
```

The plots are all generated by calling plot_something() where the somethings are:

* nonzero: scatter plot of number of non-zero genes with respect to CPM pseudocounts.
* libsize: bar plot of the (pseudo) counts in annotated features observed by sample.
* boxplot: boxplot describing the distribution of counts with respect to features by sample.
* corheat: correlation heat map describing the relative similarities among samples.
* smc: 'standard median correlation', take the pairwise correlations of all samples, calculate the
  medians, and look for a single sample with significantly lower correlations (falling below the red
  line).
* disheat: distance heat map.  As above but with a distance metric.
* smd: 'standard median distance', as above but using the distance metrics and a line above.
* pcaplot: plot_pca() gives many outputs, including a PCA plot of the first 2 principal components.
* pcatable: and a table describing the same metadata.
* pcares: along with the table of variance, cumulative variance, condition/batch r^2.
* pcavar: and the variance by component observed.
* density: pretty much the exact same thing as the boxplot above, but as a density plot.
* legend: a convenient legend for figures and such.
* qqlog: qq-plots of each sample vs. the median on the log scale.
* qqrat: qq-plots of each sample vs. the median as ratios.
* ma: bland-altman plots of each sample of M(log abundance) with respect to A(mean average).

# Subsetting data

On the other hand, we might take a subset of the data to
focus on the late-log vs. early-log samples.

The expt_subset() function allows one to pull material from the
experimental design.

Once we have a smaller data set, we can more easily use PCA to see how
the sample separate.

```{r subset_data}
head(expt$design)
## elt stands for: "early/late in thy"
batch_a <- expt_subset(expt, subset="batch=='a'")
batch_b <- expt_subset(expt, subset="batch=='b'")

a_metrics <- graph_metrics(batch_a)
a_metrics$pcaplot
b_metrics <- graph_metrics(batch_b)
b_metrics$pcaplot
```

# Normalizing data

It is pretty obvious that the raw data is a bit jumbled according to PCA.  This is not paricularly
suprising since we didn't normalize it at all.  Therefore, in this block I will normalize it a few
ways and follow up with some visualizations of showing how the apparent relationships change in the
data.

```{r normalize_subset, fig.show="hide"}
## doing nothing to the data except log2 transforming it has a surprisingly large effect
norm_test <- normalize_expt(expt, transform="log2")
l2_metrics <- sm(graph_metrics(norm_test))
## a quantile normalization alone affect some, but not all of the data
norm_test <- sm(normalize_expt(expt, norm="quant"))
q_metrics <- sm(graph_metrics(norm_test))  ## q for quant, oh oh oh!
## cpm alone brings out some samples, too
norm_test <- sm(normalize_expt(expt, convert="cpm"))
c_metrics <- sm(graph_metrics(norm_test))  ## c for cpm!
## low count filtering has some effect, too
norm_test <- sm(normalize_expt(expt, filter="pofa"))
f_metrics <- sm(graph_metrics(norm_test))  ## f for filter!
## how about if we mix and match methods?
norm_test <- sm(normalize_expt(expt, transform="log2", convert="cpm", norm="quant", batch="combat_scale", filter=TRUE, batch_step=4, low_to_zero=TRUE))
## Some metrics are not very useful on (especially quantile) normalized data
norm_graphs <- sm(graph_metrics(norm_test))
```

Now lets see some of the resulting metrics, in this case I will just compare some pca plots, as they
are good at fooling our silly visual brains into seeing patterns.

```{r view_metrics}
l2_metrics$pcaplot
## Also viewable with plot_pca()$plot
## PCA plots seem (to me) to prefer log2 scale data.
q_metrics$pcaplot
## only normalizing on the quantiles leaves the data open to scale effects.
c_metrics$pcaplot
## but cpm alone is insufficient
f_metrics$pcaplot
## only filtering out low-count genes is helpful as well
norm_graphs$pcaplot
## The different batch effect testing methods have a pretty widely ranging effect on the clustering
## play with them by changing the batch= parameter to:
## "limma", "sva", "svaseq", "limmaresid", "ruvg", "combat", combatmod"
knitr::kable(norm_graphs$pcares)
## Thus we see a dramatic decrease in variance accounted for
## by batch after applying limma's 'removebatcheffect'
## (see batch.R2 here vs. above)
norm_graphs$smc
norm_graphs$disheat  ## svaseq's batch correction seems to draw out the signal quite nicely.
## It is worth noting that the wt, early log, thy, replicate c samples are still a bit weird.
```

# Performing DE analyses

This is a relatively small data set, so performing some differential expression analyses really
should not take long at all.

When performing these analyses with hpgltools, it will attempt to perform similar analyses with
limma, edgeR, and DESeq2 via the all_pairwise() function.  The most likely argument is 'model_batch'
which may be used to explicitly include/exclude a batch factor in the model, or ask it to attempt
including batch factors from sva/ruv/etc.  By default it will attempt to include a column from the
experimental design named 'batch'.

```{r de_test}
spyogenes_de <- sm(all_pairwise(expt))
## Even the lowest correlations are quite high.
```

The result of all_pairwise() is a list of the results from limma, edger, and deseq.  In addition, I
implemented a very simplistic, differential expression function named 'basic()'.  It also provides
some simple measurements of how well the various analyses agree (ergo the black and white heatmap).

Working with these separate tables can be more than a little annoying, combine_de_tables() attempts
to simplify this.  It will bring together the various tables, and if asked attempt to bring them
together into a pretty-ified excel workbook.

all_pairwise() arbitrarily performs all possible pairwise comparisons.  This is not necessarily what
one actually wishes to see.  Therefore, the argument keepers takes a list of contrasts:

```{r keeper_example}
my_keepers <- list(
    "wt_media" = c("wt_ll_cf", "wt_ll_cg"),
    "mga_media" = c("mga_ll_cf", "mga_ll_cg"))
```

In the above example, if the keepers argument to combine_de_tables() is given as my_keepers, then
the resulting table will not have the set of 6 possible comparisons, but instead will only have 2
tables named 'wt_media' and 'mga_media', which if printed to excel will be sheets named accordingly.

Because these tables can get quickly extremely large, the excel workbooks may become too big for the
zip(1) program to properly merge.  If that happens, one may either remove some(or all) of the plots
and/or have it print csv versions of the same tables.

```{r combine_test}
spyogenes_tables <- sm(combine_de_tables(spyogenes_de, excel=FALSE))
summary(spyogenes_tables)
```

Finally, extract_significant_genes() may choose 'significant' genes based upon a few metrics
including z-score vs. the distribution of logFC; a logFC cutoff, (ajusted)p-value cutoff, and/or
top/bottom n genes.

```{r sig_genes_test}
spyogenes_sig <- sm(extract_significant_genes(spyogenes_tables, excel=FALSE))
knitr::kable(head(spyogenes_sig$limma$ups[[1]]))
```

# Make pretty circos graphs

Since most of my circos graphs are for pyogenes, it is likely that the defaults are appropriate for
this particular organism.

Much(all) of the following is taken from the material in tests/testthat/test_70mga.R

```{r circos}
microbe_ids <- as.character(sm(get_microbesonline_ids("pyogenes MGAS5005")))
mgas_df <- sm(get_microbesonline_annotation(microbe_ids[[1]])[[1]])
mgas_df$sysName <- gsub(pattern="Spy_", replacement="Spy", x=mgas_df$sysName)
rownames(mgas_df) <- make.names(mgas_df$sysName, unique=TRUE)

## First make a template configuration
circos_test <- circos_prefix()
## Fill it in with the data for s.pyogenes
circos_kary <- circos_karyotype("mgas", length=1895017)
## Fill in the gene category annotations by gene-strand
circos_plus <- circos_plus_minus(mgas_df, circos_test)

circos_limma_hist <- circos_hist(spyogenes_de$limma$all_tables[[1]], mgas_df, circos_test, outer=circos_plus)
circos_deseq_hist <- circos_hist(spyogenes_de$deseq$all_tables[[1]], mgas_df, circos_test, outer=circos_limma_hist)
circos_edger_hist <- circos_hist(spyogenes_de$edger$all_tables[[1]], mgas_df, circos_test, outer=circos_deseq_hist)
circos_suffix(cfgout=circos_test)
circos_made <- circos_make(target="mgas")
## For some reason this fails weirdly when not run interactively.
```

![circos result](circos/mgas.svg)

GenoplotR is new to me, but it seems to work?

```{r genoplot}
genoplot_chromosome()
```

[index.html](index.html)

```{r sysinfo, results='asis'}
pander::pander(sessionInfo())
```
